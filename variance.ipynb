{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating mean and variance through back propagation.\n",
    "\n",
    "Although deep learning typically solves classification problems, it is still a powerful approach\n",
    "regression problems. As any entry level ML course teaches you, the loss function for regression\n",
    "is MSE (mean squared error). \n",
    "\n",
    "One of the core reasons MSE is the right loss is because it assumes the distribution of errors follows\n",
    "a normal distribution, and that the variance (or standard deviation) of those errors is constant. The former is a good assumption because the CLT (central limit theorem) sort of says that the \"average\" distribution is the normal distribution. \n",
    "\n",
    "Looking at the PDF (probability distribution function) of a Normal distribution\n",
    "\n",
    "$$\n",
    "f(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "we see hidden in there the square of the delta between the mean and a datapoint: ${(x - \\mu)^2}$. That's actually the heart of why we use the MSE: when estimating the mean $\\mu$, when we see a datapoint $x$, we consider that an Error, then we Square it, then we just take the Mean of all the datapoints. That's how we end up using MSE. (Note that $x$ in this case represents the target value, which is a label in your dataset and typically called y_true in the context of training the parameters of a neural network, and mu is the neural network's estimate of the mean of the y value, called y_pred or y_hat)\n",
    "\n",
    "Why can we ignore the variance or $\\sigma^2$? Because an assumption is that the variance of the error is constant. That's called \"homoscedastic\" - homo meaning the same like in homonym. So when we calculate the gradient, we can ignore any constant value. \n",
    "\n",
    "What about that exponent function? Well, training neural networks is typically based on MLE (Maximum Likelihood Estimation), which says that whatever parameters give you the highest joint probability matching the output are the most likely params. Joint probability involves taking the product of lots of small numbers - for numerical stability you can log the whole thing to make it a sum of logs instead, that's one reason we find the log function in the classic crossentropy loss used for classification.\n",
    "\n",
    "$$\n",
    "\\text{Cross-Entropy Loss} = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
    "$$\n",
    "\n",
    "It's also why MSE doesn't have any pesky exponent functions - a log is applied to it, and the log and exponent basically cancel each other out. \n",
    "\n",
    "If the variance is not constant then the data has \"heteroscedasticity\" - hetero means different, like in heterogeneous, and we actually run into lots of problems when trying to do things like t-tests, but that's not the focus of this article.\n",
    "\n",
    "The focus is how to get a back-propped model to estimate variance. The answer is simply to output two heads - one for mean of our y value, one for variance, and undo the two shortcuts we applied above (constant variance, log of exponent). Based on MLE, we still need the NLL (negative of the log of the likelihood). And the likelihood is mathematically the same thing as the probability according to the frequentistic statistics behind MLE. And the probability is given by the PDF. \n",
    "\n",
    "Basically, all we have to do is the $-\\log(\\text{pdf})$, where $\\text{pdf}$ is a function estimating the probability of the prediction being x given the parameters mean and variance. The form you'll see this in ML textbooks will be something like the following, which means the loss function is the log of the pdf of an example value \n",
    "\n",
    "$$\n",
    "-\\log(\\text{pdf}(y_true | \\theta))\n",
    "$$\n",
    "\n",
    "where $\\theta$ represents the predictions of your neural network, namely, mean and variance. It's really that simple.\n",
    "\n",
    "And just to be explicit: -log(pdf) simplifies to MSE when the variance is constant. That's why the use of MSE is the direct outcome of applying MLE when variance is constant. \n",
    "\n",
    "Some of the first ideas here came from econometrics, where they wanted to estimate time-series data but realizes that during some periods of time, there is higher variance than others. This [book](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=db869fa192a3222ae4f2d766674a378e47013b1b) talks about it further in the context of ML, and of course we have to go to Fisher for MLE and Gauss and Laplace for CLT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, setup some data with different variances. \n",
    "\n",
    "We'll train a model to estimate the expected value and variance of the output of a function, based on the input. \n",
    "\n",
    "We'll have two clusters of data. If the input is between zero and one, the output is a normal distribution with mean 1 and stdev 1 (var 1).\n",
    "If the input is between ten and eleven, the output is a normal distribution with mean -1 and stdev 2 (var 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x1 = torch.randint(0, 1, (10000, 1)).float()\n",
    "y1 = torch.randn(10000, 1) + 1\n",
    "\n",
    "x2 = torch.randint(10, 11, (10000, 1)).float()\n",
    "y2 = torch.randn(10000, 1) * 2 -1\n",
    "\n",
    "x = torch.cat([x1, x2])\n",
    "y = torch.cat([y1, y2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's setup a data loader for that data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(Data(x, y), batch_size=100, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now setup a model.\n",
    "\n",
    "Nothing fancy here. Just an MLP (multilayer perceptron) with two heads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Predictor, self).__init__()\n",
    "\n",
    "        hidden_depth = 20\n",
    "\n",
    "        self.dense1 = nn.Linear(1, hidden_depth)\n",
    "        self.tanh1 = nn.Tanh()\n",
    "        self.dense2 = nn.Linear(hidden_depth, hidden_depth)\n",
    "        self.tanh2 = nn.Tanh()\n",
    "        self.dense3 = nn.Linear(hidden_depth, hidden_depth)\n",
    "        self.tanh3 = nn.Tanh()\n",
    "        self.dense4 = nn.Linear(hidden_depth, hidden_depth)\n",
    "        self.tanh4 = nn.Tanh()\n",
    "        self.dense5 = nn.Linear(hidden_depth, hidden_depth)\n",
    "\n",
    "        self.mean_y = nn.Linear(hidden_depth, 1)\n",
    "        self.var_y = nn.Linear(hidden_depth, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.tanh1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.tanh2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.tanh3(x)\n",
    "        x = self.dense4(x)\n",
    "        x = self.tanh4(x)\n",
    "        x = self.dense5(x)\n",
    "\n",
    "\n",
    "        mean = self.mean_y(x)\n",
    "        var = torch.exp(self.var_y(x)) # var should be positive\n",
    "        return mean, var\n",
    "\n",
    "model = Predictor()\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -1.0, +1.0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's setup that fancy loss function\n",
    "\n",
    "Again, it's not fancy at all, it's just NLL applied to both mean and variance of the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.8573625087738037\n",
      "Epoch 10, Loss: 1.7618536949157715\n",
      "Epoch 20, Loss: 1.8004021644592285\n",
      "Epoch 30, Loss: 1.920356273651123\n",
      "Epoch 40, Loss: 1.7423737049102783\n",
      "Epoch 50, Loss: 1.7630614042282104\n",
      "Epoch 60, Loss: 1.6256223917007446\n",
      "Epoch 70, Loss: 1.7487053871154785\n",
      "Epoch 80, Loss: 1.7681242227554321\n",
      "Epoch 90, Loss: 1.7784992456436157\n"
     ]
    }
   ],
   "source": [
    "class MeanVarianceLoss(nn.Module):\n",
    "    \"\"\"Calculates the negative log likelihood of seeing a target value given a mean and a variance.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(MeanVarianceLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, mean, var, target):\n",
    "        normal = torch.distributions.Normal(mean, torch.sqrt(var))\n",
    "\n",
    "        # For numerical stability, the torch distributions library\n",
    "        # returns the log(pdf(target | mean, variance)), not the pdf(target | mean, variance) directly.\n",
    "        log_prob = normal.log_prob(target)\n",
    "\n",
    "        # NLL is the negative log probability\n",
    "        nll = -log_prob\n",
    "\n",
    "        return torch.mean(nll)\n",
    "\n",
    "criterion = MeanVarianceLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1000.)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(100):\n",
    "    if 0 <= epoch < 20:\n",
    "        optimizer.param_groups[0]['lr'] = 1e-3\n",
    "    elif 20 <= epoch < 40:\n",
    "        optimizer.param_groups[0]['lr'] = 1e-4\n",
    "    elif 40 <= epoch < 60:\n",
    "        optimizer.param_groups[0]['lr'] = 1e-5\n",
    "    elif 60 <= epoch < 80:\n",
    "        optimizer.param_groups[0]['lr'] = 1e-6\n",
    "    else:\n",
    "        optimizer.param_groups[0]['lr'] = 1e-8\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        mean, var = model(x)\n",
    "        loss = criterion(mean, var, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0: \n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Did it work? Let's spot check it's estimate of mean and stdev on some numbers from our two sets.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimate of mean and var for first cluster of data should be mean=1, var=1.\n",
      "Mean estimate: 0.6582908034324646, Var estimate: 1.3874123096466064\n",
      "\n",
      "Estimate of mean and var for second cluster of data should be mean=-1, var=4.\n",
      "Mean estimate: -1.0008701086044312, Var estimate: 3.9981565475463867\n"
     ]
    }
   ],
   "source": [
    "mean, var = model(torch.tensor([0.5]))\n",
    "print(f\"Estimate of mean and var for first cluster of data should be mean=1, var=1.\")\n",
    "print(f\"Mean estimate: {mean.item()}, Var estimate: {var.item()}\")\n",
    "print()\n",
    "\n",
    "mean, var = model(torch.tensor([10.5]))\n",
    "print(f\"Estimate of mean and var for second cluster of data should be mean=-1, var=4.\")\n",
    "print(f\"Mean estimate: {mean.item()}, Var estimate: {var.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

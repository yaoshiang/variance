{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating mean and variance through back propagation.\n",
    "\n",
    "Although deep learning typically solves classification problems, it is still a powerful approach\n",
    "regression problems. As any entry level ML course teaches you, the loss function for regression\n",
    "is MSE (mean squared error). \n",
    "\n",
    "One of the core reasons MSE is the right loss is because it assumes the distribution of errors follows\n",
    "a normal distribution, and that the variance (or standard deviation) of those errors is constant. This is a good assumption because the CLT (central limit theorem) sort of says that the \"average\" distribution is the normal distribution. \n",
    "\n",
    "Looking at the PDF (probability distribution function) of a Normal distribution:\n",
    "\n",
    "$$\n",
    "f(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "We see hidden in there the square of the error: ${(x - \\mu)^2}$. That's why we use the Squared Error, and then we just take the Mean of all the examples. That's how we end up using MSE. \n",
    "\n",
    "Why can we ignore the variance or $\\sigma^2$? Because an assumption is that the variance of the error is constant. That's called \"homoscedastic\" - homo meaning the same like in homonym. So when we calculate the gradient, we can ignore any constant value. \n",
    "\n",
    "What about that exponent function? Well, training neural networks is typically based on MLE (Maximum Likelihood Estimation), which says that whatever parameters give you the highest joint probability matching the output are the most likely params. Joint probability involves taking the product of lots of small numbers - for numerical stability you can log the whole thing to make it a sum instead, that's one reason we find the log function in the classic crossentropy loss used for classification.\n",
    "\n",
    "$$\n",
    "\\text{Cross-Entropy Loss} = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
    "$$\n",
    "\n",
    "It's also why MSE doesn't have any pesky exponent functions - a log is applied to it, and the log and exponent basically cancel each other out. \n",
    "\n",
    "If the variance is not constant then the data has \"heteroscedasticity\" - hetero means different, like in heterogeneous, and we actually run into lots of problems when trying to do things like t-tests, but that's not the focus of this article.\n",
    "\n",
    "The focus is how to get a back-propped model to estimate variance. The answer is simply to output two heads - one for mean, one for variance, and undo the two shortcuts we applied above (constant variance, log of exponent). Basically, all we have to do is the NLL of the PDF ((negative of the log of the likelihood of the probability distribution function, where probability becomes likelihood when we apply MLE). Again: it's just -log(pdf(x | mean & var)). It's really that simple.\n",
    "\n",
    "And just to be explicit: -log(pdf) simplifies to MSE when the variance is constant. That's why the use of MSE is perfectly consistent with MLE and it's use of NLL.  \n",
    "\n",
    "Some of the first ideas here came from econometrics, where they wanted to estimate time-series data but realizes that during some periods of time, there is higher variance than others. This [book](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=db869fa192a3222ae4f2d766674a378e47013b1b) talks about it further in the context of ML, and of course we have to go to Fisher for MLE and Gauss and Laplace for CLT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, setup some data with different variance. \n",
    "\n",
    "We'll train a model to estimate the expected value and variance of the output of a function, based on the input. \n",
    "\n",
    "We'll have two clusters of data. If the input is between zero and one, the output is a normal distribution with mean 1 and stdev 1 (var 1).\n",
    "If the input is between ten and eleven, the output is a normal distribution with mean -1 and stdev 2 (var 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x1 = torch.randint(0, 1, (10000, 1)).float()\n",
    "y1 = torch.randn(10000, 1) + 1\n",
    "\n",
    "x2 = torch.randint(10, 11, (10000, 1)).float()\n",
    "y2 = torch.randn(10000, 1) * 2 -1\n",
    "\n",
    "x = torch.cat([x1, x2])\n",
    "y = torch.cat([y1, y2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's setup a data loader for that data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(Data(x, y), batch_size=100, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now setup a model and loss function: NLL(PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.5946478843688965\n",
      "Epoch 10, Loss: 1.9665488004684448\n",
      "Epoch 20, Loss: 1.7211822271347046\n",
      "Epoch 30, Loss: 1.8307368755340576\n",
      "Epoch 40, Loss: 1.676632046699524\n",
      "Epoch 50, Loss: 1.7625548839569092\n",
      "Epoch 60, Loss: 1.8011854887008667\n",
      "Epoch 70, Loss: 1.6816025972366333\n",
      "Epoch 80, Loss: 1.6646822690963745\n",
      "Epoch 90, Loss: 1.7892930507659912\n"
     ]
    }
   ],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Predictor, self).__init__()\n",
    "\n",
    "        self.dense1 = nn.Linear(1, 10)\n",
    "        self.tanh1 = nn.Tanh()\n",
    "        self.dense2 = nn.Linear(10, 10)\n",
    "        self.tanh2 = nn.Tanh()\n",
    "        self.dense3 = nn.Linear(10, 10)\n",
    "        self.tanh3 = nn.Tanh()\n",
    "        self.dense4 = nn.Linear(10, 10)\n",
    "        self.tanh4 = nn.Tanh()\n",
    "        self.dense5 = nn.Linear(10, 10)\n",
    "\n",
    "        self.mean_y = nn.Linear(10, 1)\n",
    "        self.var_y = nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.tanh1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.tanh2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.tanh3(x)\n",
    "        x = self.dense4(x)\n",
    "        x = self.tanh4(x)\n",
    "        x = self.dense5(x)\n",
    "\n",
    "\n",
    "        mean = self.mean_y(x)\n",
    "        var = torch.exp(self.var_y(x)) + 1e-2 # Variance is always positive, so we activate with the exp function, not sigmoid/softmax.\n",
    "        return mean, var\n",
    "\n",
    "model = Predictor()\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -1.0, +1.0))\n",
    "\n",
    "\n",
    "class MeanVarianceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanVarianceLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, mean, var, target):\n",
    "        normal = torch.distributions.Normal(mean, torch.sqrt(var))\n",
    "\n",
    "        # For numerical stability, the torch distributions library\n",
    "        # returns the log(pdf(target | mean, variance)), not the pdf(target | mean, variance) directly.\n",
    "        log_prob = normal.log_prob(target)\n",
    "\n",
    "        # NLL is the negative log probability\n",
    "        nll = -log_prob\n",
    "\n",
    "        return torch.mean(nll)\n",
    "\n",
    "criterion = MeanVarianceLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1000.)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(100):\n",
    "    if 0 <= epoch < 20:\n",
    "        optimizer.param_groups[0]['lr'] = 1e-3\n",
    "    elif 20 <= epoch < 40:\n",
    "        optimizer.param_groups[0]['lr'] = 1e-4\n",
    "    elif 40 <= epoch < 60:\n",
    "        optimizer.param_groups[0]['lr'] = 1e-5\n",
    "    elif 60 <= epoch < 80:\n",
    "        optimizer.param_groups[0]['lr'] = 1e-6\n",
    "    else:\n",
    "        optimizer.param_groups[0]['lr'] = 1e-7  \n",
    "\n",
    "    for x, y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        mean, var = model(x)\n",
    "        loss = criterion(mean, var, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0: \n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Did it work? Let's spot check it's estimate of mean and stdev on some numbers from our two sets.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimate of mean and var for x = 1 should be mean=1, var=1. Mean estimate: 0.8190476298332214, Var estimate: 1.3436105251312256\n",
      "Estimate of mean and var for x = 10.5 should be mean=-1, var=4. Mean estimate: -0.8588033318519592, Var estimate: 4.115914344787598\n"
     ]
    }
   ],
   "source": [
    "mean, var = model(torch.tensor([0.5]))\n",
    "print(f\"Estimate of mean and var for x = 1 should be mean=1, var=1. Mean estimate: {mean.item()}, Var estimate: {var.item()}\")\n",
    "\n",
    "mean, var = model(torch.tensor([10.5]))\n",
    "print(f\"Estimate of mean and var for x = 10.5 should be mean=-1, var=4. Mean estimate: {mean.item()}, Var estimate: {var.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
